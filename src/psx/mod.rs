//! Top level for the emulation code. Contains the state of the emulated console.

pub mod bios;
pub mod cd;
pub mod cop0;
pub mod cpu;
pub mod cpu_instructions;
#[cfg(feature = "debugger")]
pub mod debugger;
mod cache;
mod dma;
mod expansion;
pub mod gpu;
mod gte;
mod irq;
mod link_cable;
mod mdec;
mod memory_control;
pub mod memory_map;
pub mod overlay;
pub mod pad_memcard;
mod spu;
mod sync;
mod timers;
mod xmem;

use crate::error::{PsxError, Result};
pub use cd::{disc, iso9660, CDC_ROM_SHA256, CDC_ROM_SIZE};
pub use gpu::{Frame, VideoStandard};
pub use overlay::{DeveloperOverlay, renderer::OverlayRenderData};
pub use spu::SpuDebugOverlay;
use serde::de::{Deserialize, Deserializer};
use std::cmp::min;

/// Type alias used to represent a number of clock cycles
pub type CycleCount = i32;

/// Current state of the emulator
#[derive(serde::Serialize, serde::Deserialize)]
pub struct Psx {
    /// Counter of the number of CPU cycles elapsed since an arbitrary point in time. Used as the
    /// reference to synchronize the other modules
    cycle_counter: CycleCount,
    /// Set to true when the GPU is done drawing one frame (or one field in interlaced mode)
    frame_done: bool,
    sync: sync::Synchronizer,
    pub cpu: cpu::Cpu,
    pub cop0: cop0::Cop0,
    pub gte: gte::Gte,
    irq: irq::InterruptState,
    /// Executable memory: RAM, Bios etc...
    xmem: xmem::XMemory,
    scratch_pad: ScratchPad,
    pub spu: spu::Spu,
    dma: dma::Dma,
    timers: timers::Timers,
    pub gpu: gpu::Gpu,
    mdec: mdec::MDec,
    pub cd: cd::CdInterface,
    pub pad_memcard: pad_memcard::PadMemCard,
    /// Expansion port controller
    expansion: expansion::ExpansionPort,
    /// Link cable controller
    link_cable: link_cable::LinkCable,
    /// Enhanced cache system
    cache_system: cache::CacheSystem,
    /// Enhanced memory control
    memory_ctrl: memory_control::MemoryControl,
    /// Memory control registers (legacy)
    mem_control: [u32; 9],
    /// Contents of the RAM_SIZE register which is probably a configuration register for the memory
    /// controller.
    ram_size: u32,
    /// Contents of the CACHE_CONTROL register
    cache_control: u32,
    /// Used to simulate the CPU slowdown generated by DMA operation
    dma_timing_penalty: CycleCount,
    /// When this variable is `true` the CPU is stopped for DMA operation
    cpu_stalled_for_dma: bool,
    /// Developer overlay system (not serialized)
    #[serde(skip)]
    developer_overlay: overlay::DeveloperOverlay,
}

impl Psx {
    pub fn new_with_disc(
        disc: disc::Disc,
        bios: bios::Bios,
        cdc_firmware: [u8; cd::CDC_ROM_SIZE],
    ) -> Result<Psx> {
        let standard = disc.region().video_standard();

        Psx::new_with_bios(Some(disc), bios, standard, cdc_firmware)
    }

    pub fn new_with_bios(
        disc: Option<disc::Disc>,
        bios: bios::Bios,
        standard: gpu::VideoStandard,
        cdc_firmware: [u8; cd::CDC_ROM_SIZE],
    ) -> Result<Psx> {
        let mut xmem = xmem::XMemory::new();

        xmem.set_bios(bios.get_rom());

        let cd = cd::CdInterface::new(disc, cdc_firmware)?;

        Ok(Psx {
            cycle_counter: 0,
            frame_done: false,
            sync: sync::Synchronizer::new(),
            cpu: cpu::Cpu::new(),
            cop0: cop0::Cop0::new(),
            gte: gte::Gte::new(),
            irq: irq::InterruptState::new(),
            xmem,
            scratch_pad: ScratchPad::new(),
            spu: spu::Spu::new(),
            dma: dma::Dma::new(),
            timers: timers::Timers::new(),
            gpu: gpu::Gpu::new(standard),
            mdec: mdec::MDec::new(),
            cd,
            pad_memcard: pad_memcard::PadMemCard::new(),
            expansion: expansion::ExpansionPort::new(),
            link_cable: link_cable::LinkCable::new(),
            cache_system: cache::CacheSystem::new(),
            memory_ctrl: memory_control::MemoryControl::new(),
            mem_control: [0; 9],
            ram_size: 0,
            cache_control: 0,
            dma_timing_penalty: 0,
            cpu_stalled_for_dma: false,
            developer_overlay: overlay::DeveloperOverlay::new(),
        })
    }

    /// Deserialize a Psx from the given object and transfer our disc, BIOS and CDC ROM to it, then
    /// replace our current state with the deserialized one.
    ///
    /// Will generate an error if there's a mismatch (that is, if the serialized Psx used a
    /// different disc, BIOS or ROM). In this case the state remains unchanged.
    pub fn deserialize_and_load<'de, D>(&mut self, reader: D) -> Result<()>
    where
        D: Deserializer<'de>,
    {
        let mut psx = Box::<Psx>::deserialize(reader)
            .map_err(|e| PsxError::DeserializationError(format!("{}", e)))?;

        psx.xmem.copy_bios(&self.xmem)?;
        psx.cd.cdc.copy_rom(&self.cd.cdc);

        if let Err((e, disc)) = psx.cd.cdc.set_disc(self.cd.cdc.take_disc()) {
            // Take the disc back
            self.cd
                .cdc
                .set_disc(disc)
                .map_err(|(e, _d)| e)
                .expect("Couldn't reload the disc!");

            return Err(e);
        }

        // Move pads to the new instance
        {
            let mut old_pads = self.pad_memcard.gamepads_mut();
            let mut new_pads = psx.pad_memcard.gamepads_mut();

            for (old, new) in old_pads.iter_mut().zip(new_pads.iter_mut()) {
                let pad = old.disconnect_device();
                new.connect_device(pad);
            }
        }

        // Move memory cards to the new instance
        {
            let mut old_mc = self.pad_memcard.memory_cards_mut();
            let mut new_mc = psx.pad_memcard.memory_cards_mut();

            for (old, new) in old_mc.iter_mut().zip(new_mc.iter_mut()) {
                let mc = old.disconnect_device();
                new.connect_device(mc);
            }
        }

        *self = *psx;

        Ok(())
    }

    pub fn video_standard(&self) -> VideoStandard {
        self.gpu.video_standard()
    }

    /// Run the emulator for a single frame
    pub fn run_frame(&mut self) {
        self.frame_done = false;
        while !self.frame_done {
            if self.cpu_stalled_for_dma {
                // Fast forward to the next event
                self.cycle_counter = self.sync.first_event();
            } else {
                while !sync::is_event_pending(self) {
                    cpu::run_next_instruction(self);
                }
            }

            sync::handle_events(self);
        }

        // Update developer overlay if enabled
        if self.developer_overlay.is_enabled() {
            self.update_developer_overlay(self.cycle_counter);
        }

        // Rebase the event counters relative to the cycle_counter to make sure they don't overflow
        sync::rebase_counters(self);
    }

    pub fn take_frame(&mut self) -> Option<Frame> {
        self.gpu.take_frame()
    }

    /// Get pending audio samples since the last call to `clear_audio_samples`
    pub fn get_audio_samples(&mut self) -> &[i16] {
        spu::get_samples(self)
    }

    /// Clear any pending audio samples. This must be called at least once per frame.
    pub fn clear_audio_samples(&mut self) {
        spu::clear_samples(self)
    }

    /// Set the internal resolution upscaling factor
    /// 0 = 1x (native), 1 = 2x, 2 = 4x, etc.
    pub fn set_upscale_shift(&mut self, shift: u8) {
        self.gpu.set_upscale_shift(shift);
    }

    /// Enable or disable SPU reverb
    pub fn set_spu_reverb_enable(&mut self, enable: bool) {
        self.spu.set_reverb_enable(enable);
    }

    /// Enable or disable enhanced SPU reverb mode for better audio quality
    pub fn set_spu_reverb_enhanced(&mut self, enhanced: bool) {
        self.spu.set_reverb_enhanced(enhanced);
    }

    /// Enable or disable SPU debug overlay
    pub fn enable_spu_debug_overlay(&mut self, enable: bool) {
        self.spu.enable_debug_overlay(enable);
    }

    /// Get SPU debug overlay data if enabled
    pub fn get_spu_debug_overlay(&self) -> Option<&spu::SpuDebugOverlay> {
        self.spu.get_debug_overlay()
    }

    /// Enable or disable the developer overlay
    pub fn set_developer_overlay_enabled(&mut self, enabled: bool) {
        self.developer_overlay.set_enabled(enabled);
    }

    /// Check if developer overlay is enabled
    pub fn is_developer_overlay_enabled(&self) -> bool {
        self.developer_overlay.is_enabled()
    }

    /// Get the developer overlay for configuration
    pub fn developer_overlay_mut(&mut self) -> &mut overlay::DeveloperOverlay {
        &mut self.developer_overlay
    }

    /// Get the developer overlay render data
    pub fn get_developer_overlay_render_data(&self) -> overlay::renderer::OverlayRenderData {
        let renderer = overlay::renderer::OverlayRenderer::new();
        renderer.render(&self.developer_overlay)
    }

    /// Update developer overlay metrics
    fn update_developer_overlay(&mut self, elapsed_cycles: CycleCount) {
        self.developer_overlay.update(self, elapsed_cycles);
    }

    /// Advance the CPU cycle counter by the given number of ticks
    fn tick(&mut self, cycles: CycleCount) {
        self.cycle_counter += cycles;
    }

    /// Like load, but tries to minimizes side-effects. Used for debugging.
    #[cfg(feature = "debugger")]
    pub fn examine<T: Addressable>(&mut self, address: u32) -> T {
        // A bit heavy handed but that shouldn't pose much of a problem since this function should
        // only be used for debugging. Catching unwinds means that it will be harder for the
        // debugger to crash the emulator if it triggers an unhandled edge case (in particular if
        // it attempts to read from some unimplemented memory location).
        use ::std::panic;

        let cc = self.cycle_counter;

        let result = panic::catch_unwind(panic::AssertUnwindSafe(|| self.load(address)));

        // Restore the previous counter to avoid advancing the emulation state with debug reads
        self.cycle_counter = cc;

        let bad_value = Addressable::from_u32(0xfbad_c0de);

        result.unwrap_or(bad_value)
    }

    /// Decode `address` and perform the load from the target module. `cc` contains the value of
    /// the CPU cycle counter when the transfer begins and should be updated to contain the value
    /// of the cycle counter when it'll complete.
    fn load<T: Addressable>(&mut self, address: u32) -> T {
        let abs_addr = map::mask_region(address);

        // XXX Shouldn't we set dma_timing_penalty to 0 once we've "ticked" it? Mednafen doesn't do
        // it but I don't understand why not. Maybe it's just that the DMA is updated often enough
        // that it doesn't matter because the timing penalty is updated constantly?
        self.tick(self.dma_timing_penalty);

        if let Some(offset) = map::RAM.contains(abs_addr) {
            self.tick(3);
            return self.xmem.ram_load(offset);
        }

        if let Some(offset) = map::BIOS.contains(abs_addr) {
            // XXX Mednafen doesn't add any penalty for BIOS read, which sounds wrong. It's
            // probably not a common-enough occurence to matter
            return self.xmem.bios_load(offset);
        }

        if let Some(offset) = map::SPU.contains(abs_addr) {
            if T::width() == AccessWidth::Word {
                self.tick(36);
            } else {
                self.tick(16);
            }

            return spu::load(self, offset);
        }

        if let Some(offset) = map::DMA.contains(abs_addr) {
            self.tick(1);
            return dma::load(self, offset);
        }

        if let Some(offset) = map::TIMERS.contains(abs_addr) {
            self.tick(1);
            return timers::load(self, offset);
        }

        if let Some(offset) = map::GPU.contains(abs_addr) {
            self.tick(1);
            return gpu::load(self, offset);
        }

        if let Some(offset) = map::MDEC.contains(abs_addr) {
            self.tick(1);
            return mdec::load(self, offset);
        }

        if let Some(offset) = map::PAD_MEMCARD.contains(abs_addr) {
            self.tick(1);
            // TODO
            return pad_memcard::load(self, offset);
        }

        if let Some(offset) = map::CDROM.contains(abs_addr) {
            self.tick(6 * T::width() as i32);
            return cd::load(self, offset);
        }

        if let Some(off) = map::IRQ_CONTROL.contains(abs_addr) {
            self.tick(1);

            let v = match off {
                0 => u32::from(irq::status(self)),
                4 => u32::from(irq::mask(self)),
                _ => {
                    warn!("Unhandled IRQ load at address {:08x}, returning 0", abs_addr);
                    0
                }
            };

            // Since the IRQ registers are only 16bit wide the high 32bits are undefined. In
            // practice the high bits appear to maintain the value of the previous load.
            //
            // We could try to emulate this behaviour by keeping track of the previously loaded
            // value and use that but it's unclear if any game relies on this edge case.
            //
            // So why 0x1f80 in the high bits?
            //
            // In the BIOS IRQ handler just before loading the value of the mask the code loads the
            // base address of the IRQ handler (0x1f801070) then does an LW of the mask (base + 4).
            // In this case the load will return 0x1f80 in the high 16 bits.
            //
            // So any code that does "load IRQ register address -> load IRQ register" in sequence
            // will have 1f80 in the high bits, so it's a sane default.
            return Addressable::from_u32(v | 0x1f80_0000);
        }

        if let Some(offset) = map::EXPANSION_1.contains(abs_addr) {
            // Expansion port region 1
            return self.expansion.load(offset);
        }

        if map::CACHE_CONTROL.contains(abs_addr).is_some() {
            if T::width() != AccessWidth::Word {
                panic!("Unhandled cache control access");
            }

            return Addressable::from_u32(self.cache_control);
        }

        if let Some(offset) = map::MEM_CONTROL.contains(abs_addr) {
            if T::width() != AccessWidth::Word {
                panic!("Unhandled MEM_CONTROL {:?} access", T::width());
            }

            let index = (offset >> 2) as usize;

            return Addressable::from_u32(self.mem_control[index]);
        }

        if map::RAM_SIZE.contains(abs_addr).is_some() {
            if T::width() != AccessWidth::Word {
                panic!("Unhandled RAM_SIZE access");
            }

            return Addressable::from_u32(self.ram_size);
        }

        // Log warning and return a safe default value instead of panicking
        warn!("Unhandled load at address {:08x}, returning 0xdeadbeef", abs_addr);
        Addressable::from_u32(0xdeadbeef)
    }

    /// Decode `address` and perform the store to the target module
    fn store<T: Addressable>(&mut self, address: u32, val: T) {
        let abs_addr = map::mask_region(address);

        if let Some(offset) = map::RAM.contains(abs_addr) {
            self.xmem.ram_store(offset, val);
            return;
        }

        if let Some(offset) = map::SCRATCH_PAD.contains(abs_addr) {
            return self.scratch_pad.store(offset, val);
        }

        if let Some(offset) = map::SPU.contains(abs_addr) {
            spu::store(self, offset, val);
            return;
        }

        if let Some(offset) = map::DMA.contains(abs_addr) {
            dma::store(self, offset, val);
            return;
        }

        if let Some(offset) = map::TIMERS.contains(abs_addr) {
            timers::store(self, offset, val);
            return;
        }

        if let Some(offset) = map::GPU.contains(abs_addr) {
            gpu::store(self, offset, val);
            return;
        }

        if let Some(offset) = map::MDEC.contains(abs_addr) {
            mdec::store(self, offset, val);
            return;
        }

        if let Some(offset) = map::PAD_MEMCARD.contains(abs_addr) {
            pad_memcard::store(self, offset, val);
            return;
        }

        if let Some(offset) = map::CDROM.contains(abs_addr) {
            cd::store(self, offset, val);
            return;
        }

        if let Some(offset) = map::IRQ_CONTROL.contains(abs_addr) {
            match offset {
                0 => irq::ack(self, val.as_u16()),
                4 => irq::set_mask(self, val.as_u16()),
                _ => {
                    warn!("Unhandled IRQ store at address {:08x}, ignoring", abs_addr);
                }
            }

            return;
        }

        if let Some(offset) = map::EXPANSION_1.contains(abs_addr) {
            // Expansion port region 1
            self.expansion.store(offset, val);
            return;
        }

        if let Some(offset) = map::MEM_CONTROL.contains(abs_addr) {
            if T::width() != AccessWidth::Word {
                panic!("Unhandled MEM_CONTROL {:?} access", T::width());
            }

            let val = val.as_u32();

            // We don't actually implement those registers, we assume that all BIOSes and games are
            // going to use the default memory configuration. I'm not aware of any game that breaks
            // this assumption. Still, we can catch any attempt at using a non-standard
            // configuration and report an error.
            match offset {
                // Expansion 1 base address
                0 => {
                    if val != 0x1f00_0000 {
                        panic!("Bad expansion 1 base address: 0x{:08x}", val);
                    }
                }
                // Expansion 2 base address
                4 => {
                    if val != 0x1f80_2000 {
                        panic!("Bad expansion 2 base address: 0x{:08x}", val);
                    }
                }
                _ => (),
            }

            let index = (offset >> 2) as usize;
            self.mem_control[index] = val;
            return;
        }

        if map::CACHE_CONTROL.contains(abs_addr).is_some() {
            if T::width() != AccessWidth::Word {
                panic!("Unhandled cache control access");
            }

            self.cache_control = val.as_u32();

            return;
        }

        if map::RAM_SIZE.contains(abs_addr).is_some() {
            if T::width() != AccessWidth::Word {
                panic!("Unhandled RAM_SIZE access");
            }

            self.ram_size = val.as_u32();
            return;
        }

        if let Some(offset) = map::EXPANSION_2.contains(abs_addr) {
            warn!("Unhandled write to expansion 2 register {:x}", offset);
            return;
        }

        panic!(
            "Unhandled store at address {:08x} (val=0x{:08x})",
            abs_addr,
            val.as_u32()
        );
    }

    /// Returns true if the instruction cache is enabled in the CACHE_CONTROL register
    fn icache_enabled(&self) -> bool {
        self.cache_control & 0x800 != 0
    }

    /// Returns true if the cache is in "tag test mode"
    fn tag_test_mode(&self) -> bool {
        self.cache_control & 4 != 0
    }

    fn set_dma_timing_penalty(&mut self, penalty: CycleCount) {
        // XXX This is from mednafen and it's hacky. Basically since we store the `free_cycles` in
        // u8 we can't stall the CPU for more than 255 cycles or it'll overflow. The reason this
        // limitation at 200 doesn't break everything is because the DMA is refreshed very often
        // and it will call this method again with the remainder of the penalty as long as it
        // exists. It's still very ugly...
        self.dma_timing_penalty = min(penalty, 200);
    }

    fn set_cpu_stalled_for_dma(&mut self, stalled: bool) {
        self.cpu_stalled_for_dma = stalled;
    }
}

/// Types of access supported by the PlayStation architecture
#[derive(PartialEq, Eq, Debug)]
pub enum AccessWidth {
    Byte = 1,
    HalfWord = 2,
    Word = 4,
}

/// rait representing the attributes of a primitive addressable
/// memory location.
pub trait Addressable {
    /// Retrieve the width of the access
    fn width() -> AccessWidth;
    /// Build an Addressable value from an u32. If the Addressable is 8 or 16bits wide the MSBs are
    /// discarded to fit.
    fn from_u32(i: u32) -> Self;
    /// Retrieve the value of the Addressable as an u32. If the Addressable is 8 or 16bits wide the
    /// MSBs are padded with 0s.
    fn as_u32(&self) -> u32;
    /// Retrieve the value of the Addressable as an u16. If the Addressable was 8 bit wide the MSBs
    /// are padded with 0s, if it was 32bit wide the MSBs are truncated.
    fn as_u16(&self) -> u16 {
        self.as_u32() as u16
    }
    /// Retrieve the value of the Addressable as an u8. If the Addressable was 16 or 32bit wide the
    /// MSBs are truncated.
    fn as_u8(&self) -> u8 {
        self.as_u32() as u8
    }
}

impl Addressable for u8 {
    fn width() -> AccessWidth {
        AccessWidth::Byte
    }

    fn from_u32(v: u32) -> u8 {
        v as u8
    }

    fn as_u32(&self) -> u32 {
        u32::from(*self)
    }
}

impl Addressable for u16 {
    fn width() -> AccessWidth {
        AccessWidth::HalfWord
    }

    fn from_u32(v: u32) -> u16 {
        v as u16
    }

    fn as_u32(&self) -> u32 {
        u32::from(*self)
    }
}

impl Addressable for u32 {
    fn width() -> AccessWidth {
        AccessWidth::Word
    }

    fn from_u32(v: u32) -> u32 {
        v
    }

    fn as_u32(&self) -> u32 {
        *self
    }
}

/// Scratch Pad (data cache used as fast RAM)
#[derive(serde::Serialize, serde::Deserialize)]
struct ScratchPad {
    #[serde(with = "serde_big_array::BigArray")]
    data: [u8; SCRATCH_PAD_SIZE],
}

impl ScratchPad {
    /// Instantiate Scratch Pad
    pub fn new() -> ScratchPad {
        ScratchPad {
            data: [0; SCRATCH_PAD_SIZE],
        }
    }

    /// Fetch the little endian value at `offset`
    pub fn load<T: Addressable>(&self, offset: u32) -> T {
        // The two MSBs are ignored, the 2MB RAM is mirrored four times over the first 8MB of
        // address space
        let offset = (offset & 0x1f_ffff) as usize;

        let mut v = 0;

        for i in 0..T::width() as usize {
            let b = u32::from(self.data[offset + i]);

            v |= b << (i * 8)
        }

        Addressable::from_u32(v)
    }

    /// Store the 32bit little endian word `val` into `offset`
    pub fn store<T: Addressable>(&mut self, offset: u32, val: T) {
        // The two MSBs are ignored, the 2MB RAM is mirrored four times over the first 8MB of
        // address space
        let offset = (offset & 0x1f_ffff) as usize;

        let val = val.as_u32();

        for i in 0..T::width() as usize {
            self.data[offset + i] = (val >> (i * 8)) as u8;
        }
    }
}

/// Scratch Pad (data cache): 1KB
const SCRATCH_PAD_SIZE: usize = 1024;

pub mod map {
    //! PlayStation memory map

    /// Mask array used to strip the region bits of the address. The mask is selected using the 3
    /// MSBs of the address so each entry effectively matches 512kB of the address space. KSEG2 is
    /// not touched since it doesn't share anything with the other regions.
    const REGION_MASK: [u32; 8] = [
        // KUSEG: 2048MB
        0xffff_ffff,
        0xffff_ffff,
        0xffff_ffff,
        0xffff_ffff,
        // KSEG0:  512MB
        0x7fff_ffff,
        // KSEG1:  512MB
        0x1fff_ffff,
        // KSEG2: 1024MB
        0xffff_ffff,
        0xffff_ffff,
    ];

    /// Mask a CPU address to remove the region bits.
    pub fn mask_region(addr: u32) -> u32 {
        // Index address space in 512MB chunks
        let index = (addr >> 29) as usize;

        addr & REGION_MASK[index]
    }

    pub struct Range(u32, u32);

    impl Range {
        /// Return `Some(offset)` if addr is contained in `self`
        pub fn contains(self, addr: u32) -> Option<u32> {
            let Range(start, length) = self;

            if addr >= start && addr < start + length {
                Some(addr - start)
            } else {
                None
            }
        }
    }

    /// Main RAM: 2MB mirrored four times over the first 8MB
    pub const RAM: Range = Range(0x0000_0000, 8 * 1024 * 1024);

    /// Expansion region 1
    pub const EXPANSION_1: Range = Range(0x1f00_0000, 512 * 1024);

    /// BIOS ROM. Read-only, significantly slower to access than system RAM
    pub const BIOS: Range = Range(0x1fc0_0000, 512 * 1024);

    /// ScratchPad: data cache used as a fast 1kB RAM
    pub const SCRATCH_PAD: Range = Range(0x1f80_0000, 1024);

    /// Memory latency and expansion mapping
    pub const MEM_CONTROL: Range = Range(0x1f80_1000, 36);

    /// Gamepad and memory card controller
    pub const PAD_MEMCARD: Range = Range(0x1f80_1040, 32);

    /// Register that has something to do with RAM configuration, configured by the BIOS
    pub const RAM_SIZE: Range = Range(0x1f80_1060, 4);

    /// Interrupt Control registers (status and mask)
    pub const IRQ_CONTROL: Range = Range(0x1f80_1070, 8);

    /// Direct Memory Access registers
    pub const DMA: Range = Range(0x1f80_1080, 0x80);

    /// Timer registers
    pub const TIMERS: Range = Range(0x1f80_1100, 0x30);

    /// CDROM controller
    pub const CDROM: Range = Range(0x1f80_1800, 4);

    /// GPU Registers
    pub const GPU: Range = Range(0x1f80_1810, 8);

    /// MDEC registers
    pub const MDEC: Range = Range(0x1f80_1820, 8);

    /// SPU registers
    pub const SPU: Range = Range(0x1f80_1c00, 640);

    /// Expansion region 2
    pub const EXPANSION_2: Range = Range(0x1f80_2000, 66);

    /// Cache control register. Full address since it's in KSEG2
    pub const CACHE_CONTROL: Range = Range(0xfffe_0130, 4);
}
